# -*- coding: utf-8 -*-
"""Car_Racing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VNvPfRBEyddSSIdeN1nHRTKh0B8gUCD8
"""

!pip install stable-baselines3
!pip install swig
!pip install gym
!apt-get install -y python-box2d
!pip install gym[box2d]

import gym
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
import imageio
import matplotlib.pyplot as plt

class PPOAgent:
    def __init__(self, state_shape, action_dim, action_bound, learning_rate=0.001, gamma=0.99, epsilon=0.2, clip_value=0.2):
        self.state_shape = state_shape
        self.action_dim = action_dim
        self.action_bound = action_bound
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.clip_value = clip_value

        self.actor = self.build_actor()
        self.critic = self.build_critic()

    def build_actor(self):
        state_input = Input(shape=self.state_shape)
        conv1 = Conv2D(32, (3, 3), activation='relu')(state_input)
        pool1 = MaxPooling2D((2, 2))(conv1)
        conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)
        pool2 = MaxPooling2D((2, 2))(conv2)
        flatten = Flatten()(pool2)
        dense1 = Dense(64, activation='relu')(flatten)
        mu_output = Dense(self.action_dim, activation='tanh')(dense1)
        mu_output = tf.keras.layers.Lambda(lambda x: x * self.action_bound)(mu_output)  # Scale output to action bounds
        std_output = Dense(self.action_dim, activation='softplus')(dense1)

        actor = Model(state_input, [mu_output, std_output])
        actor.compile(optimizer=Adam(learning_rate=self.learning_rate))
        return actor

    def build_critic(self):
        state_input = Input(shape=self.state_shape)
        conv1 = Conv2D(32, (3, 3), activation='relu')(state_input)
        pool1 = MaxPooling2D((2, 2))(conv1)
        conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)
        pool2 = MaxPooling2D((2, 2))(conv2)
        flatten = Flatten()(pool2)
        dense1 = Dense(64, activation='relu')(flatten)
        value_output = Dense(1, activation='linear')(dense1)

        critic = Model(state_input, value_output)
        critic.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')
        return critic

    def choose_action(self, state):
        mu, std = self.actor.predict(np.expand_dims(state, axis=0))
        action = np.random.normal(mu[0], std[0], size=self.action_dim)
        action = np.clip(action, -self.action_bound, self.action_bound)
        return action

    def learn(self, state, action, reward, next_state, done):
        target = reward + self.gamma * (1 - done) * self.critic.predict(np.expand_dims(next_state, axis=0))
        advantage = target - self.critic.predict(np.expand_dims(state, axis=0))

        mu_old, std_old = self.actor.predict(np.expand_dims(state, axis=0))
        old_policy = self.normal_likelihood(action, mu_old, std_old)

        # Train actor
        with tf.GradientTape() as tape:
            mu, std = self.actor(np.expand_dims(state, axis=0), training=True)
            new_policy = self.normal_likelihood(action, mu, std)
            ratio = new_policy / (old_policy + 1e-10)
            surr = ratio * advantage
            clipped_surr = tf.clip_by_value(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage
            actor_loss = -tf.reduce_mean(tf.minimum(surr, clipped_surr))
        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor.optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))

        # Train critic
        self.critic.fit(np.expand_dims(state, axis=0), target, verbose=0)

    def normal_likelihood(self, x, mu, std):
        pdf = 1.0 / np.sqrt(2 * np.pi * std**2) * np.exp(-((x - mu)**2) / (2 * std**2))
        return pdf

# Função para visualização do treinamento
def visualize_training(env, agent, episodes=10, save_video=True, max_frames_per_episode=500):
    frames = []
    rewards = []
    for episode in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        frame_count = 0
        while not done and frame_count < max_frames_per_episode:
            frames.append(env.render(mode='rgb_array'))
            action = agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)
            total_reward += reward
            state = next_state
            frame_count += 1
        rewards.append(total_reward)
        print("Episode:", episode + 1, "Total Reward:", total_reward)
    env.close()
    if save_video:
        imageio.mimsave('./training_video.mp4', frames, fps=30)

    plt.plot(rewards)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title('Reward per Episode')
    plt.show()

    return rewards

if __name__ == "__main__":
    env = gym.make("CarRacing-v2")
    state_shape = env.observation_space.shape
    action_dim = env.action_space.shape[0]
    action_bound = env.action_space.high[0]

    agent = PPOAgent(state_shape, action_dim, action_bound)

    rewards = visualize_training(env, agent)